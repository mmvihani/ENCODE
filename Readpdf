# Reading PDFs 

import os
import re
import nltk
import xml.etree.ElementTree as et
from nltk.tokenize import PunktSentenceTokenizer
from nltk.tokenize import sexpr_tokenize
from nltk.tokenize import word_tokenize 
from nltk.parse.stanford import StanfordDependencyParser

def main():

	tree = et.parse('pone.0111329.nxml')
	root = tree.getroot()
	para = []
	for item in list(root.iter('p')):
		item = list(item.itertext())
		item = ' '.join(item)
		para.append(item)
	
	token = PunktSentenceTokenizer()
	article_paragraphs = " ".join(para)
	#print(article_paragraphs)
	
	#REMOVING CITATIONS
	#paren = sexpr_tokenize(article_paragraphs)
	#for item in paren:
	#	if '(' and ')' in item:
	#		paren.remove(item)
	#article_paragraphs = ' '.join(paren)
	
	sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
	sent = " ".join(sent_detector.tokenize(article_paragraphs.strip(),realign_boundaries = False))
	sent = token.tokenize(sent)
	words = word_tokenize(article_paragraphs)
	#words = [w for w in words if len(w) > 2]
	#print(words)
	numwords = len(words)
	#print(numwords)
	fdist = nltk.probability.FreqDist(words)
	order = fdist.most_common()
	order = order[0:50]
	sentence = sent[0]
	#print(sent)
	#dependency_parser = StanfordDependencyParser(path_to_jar = path_to_jar, path_to_models_jar=path_to_models_jar)
	#result = dependency_parser.raw_parse(sentence)
	#print(result) 
		
	count = 0
	for bacterialgene in words:
		if (bacterialgene[0:2].islower() and  bacterialgene[3:(len(item)-1)].isupper()):
			count = count + 1	
			print(bacterialgene)
	print("The number of bacterial genes: " + str(count))
		
		
main()	


